{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Python/billing_data_dummied.csv', dtype={'Parent_ABA': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Unnamed: 0', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.pop('Total Volume')                         # Remove the volume column\n",
    "df.insert(len(df.columns), 'Total Volume', data)     # Add it back in as the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\G1RXK02\\AppData\\Local\\Temp\\ipykernel_23204\\1982077826.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_MS.drop(columns=['Parent_ABA', 'CYCLE_D'], inplace=True, axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_MS.drop(columns=['Parent_ABA', 'CYCLE_D'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_MS = scaler.fit_transform(df_MS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_MS[:, -1]        # Remove the volume column; set our target variable\n",
    "x = df_MS[:, :-1]       # Filter out unnecessary columns for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Use skleans TimeSeriesSplit to split our data for training\n",
    "# Assumes the data is already sorted in ascending order by date\n",
    "tss = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "for train_index, test_index in tss.split(x):\n",
    "    \n",
    "    X_train, X_test = x[train_index, :], x[test_index, :]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.90063902e-02, 1.67286211e-07, 9.02525838e-02, ...,\n",
       "       0.00000000e+00, 2.00743453e-06, 3.91516649e-03])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensors = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensors = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensors = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensors = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([94])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = len(X_train_tensors[0])\n",
    "X_train_tensors[0] = X_train_tensors[0].view(-1, 1, num_features)\n",
    "X_train_tensors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "def train_model(config):\n",
    "    \"\"\"\n",
    "    Train an LSTM model with the given hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Dictionary containing the hyperparameters for the model.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the loss of the model.\n",
    "    \"\"\"\n",
    "    # Define your model with the given hyperparameters\n",
    "    model = LSTMPredictor(num_features=config[\"num_features\"], n_hidden=config[\"n_hidden\"], num_layers=config[\"num_layers\"])\n",
    "\n",
    "    # Define your loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # Train your model\n",
    "    for epoch in range(100):  # You can adjust the number of epochs\n",
    "        for i in range(len(X_train_tensors)):\n",
    "            # Forward pass\n",
    "            seq_len = 12  # replace with actual sequence length\n",
    "            num_features = len(X_train_tensors[i]) // seq_len\n",
    "            num_features = config[\"num_features\"]\n",
    "            X_train_tensors[i] = X_train_tensors[i].view(-1, seq_len, num_features)      # Ensure it's a 3D tensor\n",
    "            outputs = model(X_train_tensors[i])\n",
    "            loss = criterion(outputs, y_train_tensors[i])\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Report the loss to Tune, which is minimized\n",
    "        tune.report(loss=loss.item())\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "config = {\n",
    "    \"num_features\": tune.choice([len(X_train_tensors[0])]),\n",
    "    \"n_hidden\": tune.choice([10, 20, 30, 40, 50]),\n",
    "    \"num_layers\": tune.choice([1, 2, 3]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "}\n",
    "\n",
    "# Run the hyperparameter tuning\n",
    "analysis = tune.run(train_model, config=config, num_samples=10)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_config = analysis.get_best_config(metric=\"loss\", mode=\"min\")\n",
    "print(best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "# Wrap your model with skorch\n",
    "model = NeuralNetClassifier(\n",
    "    module=LSTMPredictor,\n",
    "    max_epochs=100,  # Set an appropriate number of epochs\n",
    "    lr=0.001,  # Initial learning rate (you can tune this too)\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion = nn.MSELoss(),\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    ")\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_dist = {\n",
    "    'module__n_hidden': [32, 64, 128],\n",
    "    'module__num_features': [len(X_train[0])], \n",
    "    'module__num_layers': [2, 3, 4],  \n",
    "    'lr': [0.001, 0.01, 0.1],  \n",
    "    # Add other hyperparameters you want to tune\n",
    "}\n",
    "\n",
    "# Perform random grid search\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,                          # Number of random samples\n",
    "    #cv = KFold(n_splits=5, \n",
    "     #          shuffle=True, \n",
    "      #         random_state=42),        # Cross-validation folds\n",
    "    scoring='neg_mean_squared_error',   # Choose an appropriate metric\n",
    "    verbose=1,\n",
    "    n_jobs=-1,                          # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Fit the random search to your data\n",
    "random_search.fit(X_train, y = y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Evaluate the best model on your validation set\n",
    "val_accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \"\"\"\n",
    "    Train the LSTM model and report the loss to Tune.\n",
    "\n",
    "    Args:\n",
    "        config (dict): A dictionary containing the hyperparameters for training.\n",
    "    \"\"\"\n",
    "    # Define model\n",
    "    model = LSTM(input_size=1, hidden_size=int(config[\"hidden_size\"]), output_size=1, num_layers=int(config[\"num_layers\"]))\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # Train model\n",
    "    epochs = 150\n",
    "    for i in range(epochs):\n",
    "        for seq, labels in zip(x_train_tensors, y_train_tensors):\n",
    "            optimizer.zero_grad()\n",
    "            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Report the loss to Tune\n",
    "        with tune.checkpoint_dir(step=i) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=single_loss.item())\n",
    "\n",
    "# Define the search space\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"hidden_size\": tune.choice([50, 100, 200]),\n",
    "    \"num_layers\": tune.choice([1, 2, 3])\n",
    "}\n",
    "\n",
    "# Define the scheduler and reporter\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=150,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2)\n",
    "reporter = CLIReporter(metric_columns=[\"loss\", \"training_iteration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for time series forecasting.\n",
    "\n",
    "    Attributes:\n",
    "        num_classes (int): The number of output classes.\n",
    "        num_layers (int): The number of recurrent layers.\n",
    "        input_size (int): The number of expected features in the input x.\n",
    "        hidden_size (int): The number of features in the hidden state h.\n",
    "        seq_length (int): The sequence length of the time series data.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): The number of output classes.\n",
    "            num_layers (int): The number of recurrent layers.\n",
    "            input_size (int): The number of expected features in the input x.\n",
    "            hidden_size (int): The number of features in the hidden state h.\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input to the LSTM layer.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output from the LSTM layer.\n",
    "        \"\"\"\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        h_out = h_out.view(-1, self.hidden_size)\n",
    "        \n",
    "        out = self.fc(h_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all the possible combinations of Parent ABA and Produce Code\n",
    "combinations = df[['Parent_ABA', 'Service Code']].drop_duplicates()\n",
    "\n",
    "# Define a dictionary to store the best model for each combination\n",
    "best_models = {}\n",
    "\n",
    "for _, row in combinations.iterrows():\n",
    "    parent_aba = row['Parent_ABA']\n",
    "    service_code = row['Service Code']\n",
    "\n",
    "    # Filter data for this combination\n",
    "    data = df[(df['Parent_ABA'] == parent_aba) & (df['Service Code'] == service_code]['Total Volume']\n",
    "              \n",
    "    # Preprocess data and split into training and testing sets\n",
    "    # ... (insert your preprocessing and train-test split code here) ...\n",
    "\n",
    "    # Convert your training and testing sets into PyTorch tensors\n",
    "    # ... (insert your code to convert to PyTorch tensors here) ...\n",
    "\n",
    "    # Run the hyperparameter search\n",
    "    analysis = tune.run(train_model, config=config, scheduler=scheduler, progress_reporter=reporter)\n",
    "\n",
    "    # Get the best model and store it in the dictionary\n",
    "    best_trial = analysis.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "\n",
    "    best_trained_model = LSTM(input_size=1, hidden_size=int(best_trial.config[\"hidden_size\"]), output_size=1, num_layers=int(best_trial.config[\"num_layers\"]))\n",
    "    best_trained_model.load_state_dict(torch.load(os.path.join(best_trial.checkpoint.value, \"checkpoint\"))[0])\n",
    "\n",
    "    best_models[(customer_id, product_id)] = best_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def evaluate_model(models, x_test_tensors_dict, y_test_tensors_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the LSTM models on the test data.\n",
    "\n",
    "    Args:\n",
    "        models (dict): A dictionary containing the trained LSTM models.\n",
    "        x_test_tensors_dict (dict): A dictionary containing the input sequences for the test data.\n",
    "        y_test_tensors_dict (dict): A dictionary containing the target values for the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    for (customer_id, product_id), model in models.items():\n",
    "        print(f\"Evaluating model for customer_id {customer_id} and product_id {product_id}:\")\n",
    "\n",
    "        # Get the test data for this combination\n",
    "        x_test_tensors = x_test_tensors_dict[(customer_id, product_id)]\n",
    "        y_test_tensors = y_test_tensors_dict[(customer_id, product_id)]\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for seq in x_test_tensors:\n",
    "                model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                                torch.zeros(1, 1, model.hidden_layer_size))\n",
    "                predictions.append(model(seq).item())\n",
    "\n",
    "        # Compare predictions to actual values\n",
    "        for i in range(len(y_test_tensors)):\n",
    "            print(f'Predicted: {predictions[i]}, Actual: {y_test_tensors[i].item()}')\n",
    "\n",
    "        # Calculate and print the Mean Squared Error and Mean Absolute Error\n",
    "        mse = mean_squared_error(y_test_tensors, predictions)\n",
    "        mae = mean_absolute_error(y_test_tensors, predictions)\n",
    "        print(f'Mean Squared Error: {mse}, Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_models, x_test_tensors_dict, y_test_tensors_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your XGBoost model\n",
    "xgb_model = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyperparameters and their potential distributions\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': randint(3, 6),\n",
    "    'subsample': uniform(0.8, 0.2),\n",
    "    'colsample_bytree': uniform(0.8, 0.2),\n",
    "    'reg_lambda': uniform(0.1, 10.0),\n",
    "    'reg_alpha': uniform(0.1, 10.0)\n",
    "}\n",
    "\n",
    "# Perform randomized search\n",
    "randomized_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=50, cv=5)\n",
    "randomized_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_params = randomized_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyper-parameters further based on best parameters results from the random grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyperparameters and their potential values\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'reg_lambda': [0.1, 1.0, 10.0],\n",
    "    'reg_alpha': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
